{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e621022",
   "metadata": {},
   "source": [
    "# HW 7: Logisitic Regression and Decision Trees\n",
    "\n",
    "In this homework, we'll use both logistic regression and decision trees to build a model that predicts if a mushroom is poisonous and if a couple will eventually get divorced (using two different data sets).  For both scenarios, we will use training-testing data split, and use the test data to compare the sensitivity, specificiy and accuracy of the models.  After that we'll decide which model is most reliable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ced7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from sklearn import tree\n",
    "import sklearn.metrics as metrics\n",
    "import statsmodels.api as sm \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "def TwoWaySummary(x):\n",
    "    \"\"\" x must be a 2x2 table arranged T+ F-, F+ T-\"\"\"\n",
    "    a = x[[0],[0]][0]\n",
    "    b = x[[0],[1]][0]\n",
    "    c = x[[1],[0]][0]\n",
    "    d = x[[1],[1]][0]\n",
    "    \n",
    "    print(f\"sensitivity = {a/(a+c)}\\nspecificity = {d/(b+d)}\\nrelative risk = {a*(c+d)/(c*(a+b))}\\naccuracy = {(a+d)/(a+d+c+b)}\")\n",
    "    \n",
    "mushroom_train = pd.read_csv(\"mushrooms_train.csv\")\n",
    "mushroom_test = pd.read_csv(\"mushrooms_test.csv\")\n",
    "\n",
    "divorce_train = pd.read_csv(\"divorce2_train.csv\")\n",
    "divorce_test = pd.read_csv(\"divorce2_test.csv\")\n",
    "\n",
    "\n",
    "display(mushroom_train.head(5))\n",
    "divorce_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06137b7",
   "metadata": {},
   "source": [
    "## Section 1: Predicting Marital Bliss\n",
    "\n",
    "The Class indicates whether the individual is 'Married' or 'Divorced'. It is represented by a Boolean variable, where 'Married' is represented as '1' and 'Divorced' as '0'.\"\n",
    "\n",
    "Here is a block of items, Atr33 to Atr38, which are in my opinion \"negative coded\", things that if you are more likely to say Yes to these that's not good.  \n",
    "\n",
    "33.  I can use negative statements about my wife's personality during our discussions.\n",
    "34.  I can use offensive expressions during our discussions.\n",
    "35.  I can insult our discussions.\n",
    "36.  I can be humiliating when we argue.\n",
    "37.  My argument with my wife is not calm.\n",
    "38.  I hate my wife's way of bringing it up.\n",
    "\n",
    "In the tree below, I choose just the first two and used them to make a tree.\n",
    "\n",
    "**Question 1.1** The title of this decision tree is not appropriate.  The code here is kind of involved but if you look closely at it you can understand it.  To start, find the part of this code that specifies the title and change it to **Bad Attitudes that May Lead to Divorce**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier()\n",
    "\n",
    "x=divorce_train[[\"Atr33\", \"Atr34\"]]\n",
    "\n",
    "#X = pd.get_dummies(x, drop_first = True)\n",
    "\n",
    "features = np.array([\"Personality\", \"Expressions\"])  # put your feature labels here\n",
    "\n",
    "targets =[\"Divorced\", \"Married\"]  # put your target labels here\n",
    "\n",
    "y=divorce_train[\"Class\"]\n",
    "\n",
    "div_tree = dtree.fit(x, y)\n",
    "\n",
    "\n",
    "plots.figure(figsize = (15, 12))\n",
    "tree.plot_tree(div_tree,fontsize = 10,rounded = True , filled = True, class_names = targets, feature_names= features, label =\"all\");\n",
    "plots.title(\"Put your Title here\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e4e28",
   "metadata": {},
   "source": [
    "**Question 1.2** Examining this tree, all four of the \"leaves\" on the left are all labeled \"Divorced\" and all four of the right leaves are \"Married\".  In other words, this tree doesn't need to be so complicated.  \n",
    "\n",
    "*This won't do exactly what we want, but we should try it anyway to see what happens.**\n",
    "\n",
    "Copy all the code from the cell under **1.1**.  Then find the line that starts with `tree.plot_tree` and add max_depth = 2 inside the parentheses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844e256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba6d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f360bb3",
   "metadata": {},
   "source": [
    "That didn't really work out the way we expected.  It just collapsed all the leaves were wanted to prune into shriveled leaves at the end.  \n",
    "\n",
    "**Question 1.3** Copy your code from **1.1** again, but this time look for the line of code that has `dtree = tree.DecisionTreeClassifier()`.  Add max_depth = 2 inside the `DecisionTreeClassifier`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ae164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f51f2e5",
   "metadata": {},
   "source": [
    "What have we just learned?  That the options we may add to the `tree.plot_tree` change how the tree is displayed, but the tree itself is already defined by the time we get to the point of plotting it.  To change the tree, we need to go back to the `DecisionTreeClassifier`.\n",
    "\n",
    "**Question 1.4** In the cell below, pull up the documentation for `DecisionTreeClassifier`.  Type <span style = \"color:green; font-family:ubuntumono\">help</span>(tree.DecisionTreeClassifier) and run the cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3bc832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b569f7",
   "metadata": {},
   "source": [
    "**Question 1.5** Using these methods, a decision tree can be defined using one of two different criterion.  The default method is to use the gini index.  What is the other option, and how would we use it?  You can tell this by reading the help file you just pulled up.  \n",
    "\n",
    "\n",
    "1. The other method is called the Shannon entropy and we use it by putting *criterion = \"Shannon\"* in the DecisionTreeClassifier.\n",
    "\n",
    "2. The other method is called the Shannon entropy and we use it by putting *criterion = \"entropy\"* in the DecisionTreeClassifier.\n",
    "\n",
    "3. The other method is called the Montgomery bewitched and we use it by putting *criterion = \"Montgomery\"* in the DecisionTreeClassifier.\n",
    "\n",
    "4. The other method is called the Montgomery bewitched and we use it by putting *criterion = \"bewitched\"* in the DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_1_5 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7145ef9",
   "metadata": {},
   "source": [
    "**Back to discussing the model that we have**\n",
    "\n",
    "The tree that we have so-far is called `div_tree`.  The part of this line of code with `div_tree.predict(x)` use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edfa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_columns(\"Actual\", y.to_numpy(), \"Prediction\", div_tree.predict(x)).pivot(\"Prediction\", \"Actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb906f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoWaySummary(np.array([[60,3],[7,66]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258dc29",
   "metadata": {},
   "source": [
    "Those are the results for the training data.  We need to verify those results by running the testing data through the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=divorce_test[[\"Atr33\", \"Atr34\"]]\n",
    "\n",
    "#X = pd.get_dummies(x, drop_first = True)\n",
    "\n",
    "features = np.array([\"Personality\", \"Expressions\"])\n",
    "\n",
    "# put your feature labels here\n",
    "\n",
    "targets =[\"Divorced\", \"Married\"]\n",
    "\n",
    "# put your target labels here\n",
    "\n",
    "y_test=divorce_test[\"Class\"]\n",
    "\n",
    "Table().with_columns(\"Actual\", y_test.to_numpy(), \"Prediction\", div_tree.predict(x_test)).pivot(\"Prediction\", \"Actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea569408",
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoWaySummary(np.array([[35,3],[2,36]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ab99a",
   "metadata": {},
   "source": [
    "## Section 2: Logistic Regression & Marriage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797fc010",
   "metadata": {},
   "source": [
    "**Question 2.1** In the code cell below, we run a logistic regression analysis.  It turns out that one of those variables is not a significant predictor.  \n",
    "\n",
    "Which of these two variables IS a significant predictor of Divorce?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = sm.add_constant(divorce_train[[\"Atr33\", \"Atr34\"]])\n",
    "\n",
    "y_var = divorce_train[[\"Class\"]]\n",
    "\n",
    "div_log_reg1 = sm.Logit(y_var, x_vars).fit()\n",
    "\n",
    "div_log_reg1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(div_log_reg1.predict(x_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var.iloc[:,0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86011c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_columns(\"Actual\", y_var.iloc[:,0].to_numpy(), \"Prediction\", np.round(div_log_reg1.predict(x_vars))).pivot(\"Prediction\", \"Actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoWaySummary(np.array([[60,3],[8,65]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae26352",
   "metadata": {},
   "source": [
    "**Question 2.2** Rerun the same analysis as above, but eliminate the variable that is not significant.  Call the model `log_reg2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84468944",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735419ad",
   "metadata": {},
   "source": [
    "**Question 2.2** Find the confusion matrix for this model.  Use the testing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "div_log_reg2.predict(x_var_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13482ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_columns(\"Actual\", y_var_test.iloc[:,0].to_numpy(), \"Prediction\", np.round(div_log_reg2.predict(x_var_test))).pivot(\"Prediction\", \"Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70e60c",
   "metadata": {},
   "source": [
    "**Question 2.3** Find the two-way summary for this confusion matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoWaySummary(np.array([[36,2],[3,35]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148f959",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "\n",
    "Redo all of sections 1 and 2, but use Atr36 and Atr38 as the predictor variables.  \n",
    "\n",
    "Call the tree that you make `div_tree2` and call the logistic model `div_log_reg3`.\n",
    "\n",
    "**3.1** Define `div_tree2`.\n",
    "\n",
    "**3.2** Plot `div_tree2`.\n",
    "\n",
    "*If the tree is too long, without a good reason to be, be sure to go back and prune it before going forward*\n",
    "\n",
    "**3.3** Create the confusion matrix for `div_tree2` using the testing data.\n",
    "\n",
    "**3.4** Find the two-way summary for that matrix.\n",
    "\n",
    "**3.5** Define `div_log_reg3`.\n",
    "\n",
    "**3.6** Create the confusion matrix for `div_log_reg3` using the testing data.\n",
    "\n",
    "**3.7** Find the two-way summary for that matrix.  \n",
    "\n",
    "**3.8** Is there one particular model, in this case, that is better than the other.  Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2727f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do 3.1 and 3.2 here\n",
    "## the graph is sufficient to know you did both parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do 3.3 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecac987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do 3.4 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do 3.5 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2923f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do 3.6 here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000963e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do 3.7 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfbf67",
   "metadata": {},
   "source": [
    "*Write you answer to 3.8 here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd470ff",
   "metadata": {},
   "source": [
    "## Section 4: Mushrooms\n",
    "\n",
    "In the next section, we're going to build a predictive model for whether a mushroom is edible.  \n",
    "\n",
    "To get us started, we see a fully built model that uses just three of the available predictors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0afe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11a799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db76e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_train.head(5)\n",
    "\n",
    "x=mushroom_train[['CapShape',\n",
    " 'CapSurface',\n",
    " 'CapColor']]\n",
    "\n",
    "\n",
    "## it's smart to make the test data sets now, when we have the relevant variables in front of our eyes\n",
    "x_test = mushroom_test[['CapShape',\n",
    " 'CapSurface',\n",
    " 'CapColor']]\n",
    "\n",
    "X = pd.get_dummies(x, drop_first = True)\n",
    "\n",
    "X_test = pd.get_dummies(x_test, drop_first = True)\n",
    "\n",
    "y=mushroom_train[\"Poisonous\"]\n",
    "\n",
    "y_test = mushroom_test[\"Poisonous\"]\n",
    "\n",
    "display(X.head(5))\n",
    "\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df8b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier()\n",
    "mush_tree_fit = dtree.fit(X, y)\n",
    "\n",
    "targets= [\"Edible\", \"Poisonous\"]\n",
    "\n",
    "plots.figure(figsize = (20,20))\n",
    "tree.plot_tree(mush_tree_fit, fontsize = 16,rounded = True , filled = True, class_names = targets, label =\"all\");\n",
    "plots.suptitle(\"Should I Eat This Mushroom I Found?\", size=48)\n",
    "plots.title(\"I don't care what this tree says, the answer is always 'No!'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245a011",
   "metadata": {},
   "source": [
    "That tree is horrible.  But if it works as a predictor of edibility, that's what's important.  Let's use the `.predict` method to get the final output of the decision tree.  We'll also compare the prediction with the actual and we'll eventually move on to using the testing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b52f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_columns(\"Actual\", y_test.to_numpy(), \"Prediction\", mush_tree_fit.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_columns(\"Actual\", y_test.to_numpy(), \"Prediction\", mush_tree_fit.predict(X_test)).pivot(\"Prediction\", \"Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab3667",
   "metadata": {},
   "source": [
    "**Question 4.1** Find the two-way summary that we discussed earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8077cd7",
   "metadata": {},
   "source": [
    "That's not so good.  Guess what.  If you use ALL the predictors available, you can actually build a better model.  \n",
    "\n",
    "For all of our conveniences, here's a list of all the variables in our data.  Be careful, this list includes a meaningless id number and whether target variable.  Don't use those in the predictive model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mushroom_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0daa76",
   "metadata": {},
   "source": [
    "**Question 4.2**  Using the training data, mushroom_train, build a model that uses all the variables from CapShape to Habit to predict whether the mushroom is edible or poisonous.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76152927",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982da227",
   "metadata": {},
   "source": [
    "**Question 4.3** How accurate is this model?  What is it's accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabfbf",
   "metadata": {},
   "source": [
    "# Great job.  \n",
    "\n",
    "This was a long one, but it's the last homework for this class.  Now that you've finished it, download this as a pdf and upload that pdf to D2L under HW 7.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
